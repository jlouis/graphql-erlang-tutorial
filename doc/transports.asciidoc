== Transports

GraphQL is a transport-agnostic system. It can be used on top of every
transport you can imagine. This means the GraphQL system must provide
its own way to tell the outside world about errors, since it cannot
rely on the surrounding system to do so.

The interface to GraphQL at its base needs support for sending
requests and receiving replies. There is little need for out-of-order
requests since queries tend to be large and all-encompassing.

However, newer parts of GraphQL which is currently being tried out has
support for delayed and streamed responses.footnote:[Through the use
of the `@streamed` and `@defer` directives among other things] Because
of this, GraphQL will need a more powerful transport for those kinds
of features.

.Current state
****
The current {project} implementation does not yet support the optional
tags such as `@streamed` and `@defer`. It is planned for a later
version when the basics are down and works in a robust way.
****

This tutorial implements GraphQL on top of HTTP through the use of the
Cowboy web server by Lo√Øc Hoguin. We currently use cowboy version
{cowboy-version}.

[[cowboy-handler]]
=== Cowboy Handler

To make GraphQL work with Cowboy, we use the application `sw_web`.
This application then uses `sw_core` in order to run the system. One
could imagine adding other applications to the system if you need more
transports. The web application needs a dispatcher in order to run:

[source,erlang]
----
include::{sw_web}/src/sw_web_app.erl[tags=dispatcher]
----

We could have picked any place to run the GraphQL interface, but this
code uses `/` at the root.

The Cowboy setup is straightforward, except that we manipulate a
couple of variables in order to make cowboy play better with
<<graphiql>>. Look at the file `sw_web_app.erl` for the details.

We set up the cowboy handler as a REST handler, because it is easy to
do, and because it automates a large set of things we'd like to do.
Our plan is to use content-negotiation: a web server will be served a
UI for GraphQL by default, but if the a client request comes in, we
will pass that to the GraphQL system.

The `cowboy_rest` model stems from an idea pioneered by Webmachine. We
can depict an HTTP request as a flow chart where each decision point
is a node in the chart. Since every request follows this flow chart, 
it makes sense to use a classic Erlang model: code the generic/general
parts inside a main module `cowboy_rest`, and then provide it with a
callback module. Whenever a decision node is reached, the callback
will be executed and the decision will follow the choice made by the
callback. If no callback function is present, we use a default
resolution.

==== Handler code

The handler starts by declaring the callbacks it has. Each of these
will be described in the following sections for those who are not
familiar with `cowboy_rest`:

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=exports]
----

==== Initialization & REST handling

In this section we describe how the cowboy handler is used to dispatch
a request to GraphQL. We first focus on using `cowboy_rest` to handle
the request basics so we have an easier job later on.

.init/3
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=init]
----

When cowboy dispatches to the `sw_web_graphql_handler` module, this
function is called upon initialization.

We use the _upgrade_ feature of cowboy to upgrade to the `cowboy_rest`
protocol for the remainder of the module. This means `cowboy_rest`
takes over operation and we provide callbacks to the general restful
handler for the parts we want to override.

The first thing `cowboy_rest` will do is to call the next function:

.rest_init/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=rest_init]
----

The purpose of this function is to initialize a state with relevant
information. We are passed data from the dispatcher which we store in
an Erlang map so we can refer to the information later.

.allowed_methods/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=allowed_methods]
----

This callback is used by Cowboy to figure out what the valid methods
are for this particular call. We allow *GET* and *POST* and reject any
other method, since we just want to use REST as a simple transport and
not as a full-blown system. Later we will show why we allow both.

.content_types_accepted/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=content_types_accepted]
----

What types of input we accept. The only way to execute a GraphQL query
is to provide the query embedded in a JSON document. This is currently
the way the GraphiQL tools expects its input.


.content_types_provided/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=content_types_provided]
----

The media types we can provide to a client:

* If the client requests `text/html` we will call the `to_html`
  function.
* If the client requests `application/json` we will call the `to_json`
  function.

This allows us to handle the content different depending on who is
requesting. The browser will by default ask for `text/html` which we
use to feed it a page containing <<graphiql>>. Once the GraphiQL
system is loaded into the browser, it will execute GraphQL queries by
means of setting the desired content type to `application/json`.

.charsets_provided/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=charsets_provided]
----

We only provide UTF-8. By doing so, we can simplify our backend a bit
because it doesn't have to re-encode data as long as all the data we
store are in proper UTF-8 encoding. A more advanced system would
analyze the desired content type from the client and eventually
restructure its documents to fit this desired content type. For
simplicity, we omit this part.

.resource_exists/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=resource_exists]
----

In `cowboy_rest`, the call here determines if the resource we
requested exists. Suppose, for instance, that we issue a GET request
and the resource doesn't exist. This will make cowboy return a 404 or
410 status code for the given resource. On the other hand, a POST will
use this to drive its construction of a new object in a RESTful
manner.

We need to wrangle the cowboy system a bit here. We simply call that
for any GET request, the resource exists, and for any POST request
there is a new resource we can create.

==== Processing

We now turn our attention to the actual processing of the GraphQL
query. The first case is when the client requests `text/html` in which
case we just feed data from our static part of the site:

.to_html/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=to_html]
----

Actual query processing is a bit more involved. Here is an overview of
what we need to do:

* Gather parameters. The system allows multiple ways of entering
  parameters, either as part of the URL, or as part of an input
  document.
* Split the parameters into the _query_, the _operation name_, and the
  _parameters_ for the operation.
* Parse, type check and validate the query
* Create an initial context
* Create an initial object for the cursor to point to
* Execute the query with all of the above
* Format a proper response to the client


.to_json/2 & from_json/2
[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=json_processing]
----

The main function delegates work to other functions in a top-down
fashion. In the following we will describe each of these parts. We
want the processing to work both for a POST and for a GET, so we
implement the same path on top of the functions `from_json` and
`to_json`. Again, this is a bit of wrangling of `cowboy_rest` to make
it happy with what is going on and because <<graphiql>> doesn't really
use proper RESTful invocation.

.Gathering inputs
The first thing we must do is to gather the input variables, the body
and the bindings. Then we must split those data into the _query
document_ the _operation name_ and the _parameters/variables_. The
rule, used by <<graphiql>> is that you can provide these data in the
URL as parameters or in the body. As a consequence, we must go looking
for data in multiple places

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=gather]
----

Most of this is standard operating procedure for a Cowboy application,
we just need to create the necessary helper routines for digging out
the necessary data:

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=document]
----

The `document` function searches a list of _places_ for the query
document. If found, the query document is returned.

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=variables]
----

The `variables` function carries out the same search in a list of
_places_ for the variables section. One problem here is that the
variables section, when it is part of a JSON document, can be provided
as is embedded in the JSON. Or it can be an escaped string of JSON
which has to be decoded. So we let the code handle both cases.

The function `operation_name` follows the same idea.

.Running the request
The request processing starts by a parsing step. If that step fails,
we can exit with an error. If it succeeds, we proceed by running
pre-processing. The output of the parsing step is an abstract syntax
tree.footnote:[A later version of GraphQL is likely to use abstract
binding trees instead, but for now, we run with the classic structure]

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=run_request]
----

The pre-processing step handles everything up to execution of the
Query. This step can be done once and for all for a given query and
the same query could be re-run with the same document over and over.
It corresponds to a prepared statement in the SQL world. In the
pre-processing step, we carry out all operations up until execution:

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=run_preprocess]
----
<1> Elaboration is a step which runs over the query and _annotates_
    the query with type information from the schema where possible. It
    makes the steps after this one much simpler because they can often
    look up information annotated on the abstract syntax tree.
    Elaboration also checks the general structure of the query since
    it annotates parts of the query according to the type information
    in the schema.
<2> Type checking verifies the elaborated syntax tree. It checks that
    every elaborated part of the query is well-defined with respect to
    the types which are allowed in that position. The output of the
    type checking is a new syntax tree, `AST2` on which scalar
    conversion has been run (for static variables) as an optimization;
    and a `FunEnv` which is the type scheme for each operation in the
    query.footnote:[We call operations for `functions` in the GraphQL
    system because that is what they really are, from a language
    perspective]
<3> Any query which is well-defined type wise can be executed. Yet
    those queries can still be nonsensical. If they are executed, they
    yield results which are valid responses--but they are often not
    what the client meant. Validation is a step which adds a
    "`linting`" pass on top of the system and rejects queries which
    are likely to be bad. For instance, it is checked that every
    fragment in the document has at least one use, or that a fragment
    spread expansion has at least one match. As queries grow in size
    and become automatically generated, validation becomes even more
    important.

Once pre-processing is complete, we can execute the query itself:

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=run_execute]
----
<1> Type checking of parameters is a separate operation from type
    checking the document. This is because the pre-processing of the
    document can be handled separately from running the actual query.
    Since an _operation_ in an existing document may have _variables_,
    we must type check these variables for correctness. Pre-processing
    yielded a function environment of the operations in the query. So
    we proceed by checking the `Vars` against the `FunEnv`'s type
    schema.
<2> Execution proceeds on _coerced_ variables which has been processed
    for input coercion.
<3> The `jsx` application is rather finicky with what it accepts as
    input, so we provide a wrapper which canonicalizes Erlang terms
    into JSON-valid responses (see <<response-formatter>>).
<4> In order to make `cowboy` behave, we override its normal response
    path. This gives us the processing path of `cowboy_rest` up until
    we start returning data and then we override the system in order
    to satisfy the typical response expected by <<graphiql>> and
    typical GraphQL systems.

[[response-formatter]]
.Response formatter
The `jsx` application is strict when processing Erlang terms into JSON
output. If an Erlang term does not match the JSON mapping precisely,
an error is raised, which leads to a 500 status code and `Internal
Server Error`. Since it is pretty common that errors contain data
which are not a priori JSON, we run a post-processing step on
responses on errors. We simply walk the Erlang term structure with a
transformation function which fixups the data if it is not valid JSON.

This allows a greater range of responses and everything which is valid
JSON is handled as valid JSON. In particular, it avoids a large set of
errors leading to hard-to-understand `error:badarg` failures from
`jsx`.

See the file `sw_web_response` for the formatter and encoder.

==== Errors

Central to any GraphQL system is proper error handling. We handle
errors through a helper routine in the cowboy handler which can
transform a GraphQL error into an error which GraphQL systems can
understand:

[source,erlang]
----
include::{sw_web}/src/sw_web_graphql_handler.erl[tags=errors]
----


