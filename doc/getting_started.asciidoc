[[getting_started]]

== Getting Started

This tutorial takes you through the creation of a GraphQL server
implementing the now ubiquitous _Star Wars_ schema. It is a complete
implementation of the schema given as an example in order to make it
clear how to implement a fully-fledged GraphQL server in Erlang.

The goal of the tutorial is to provide a developer with a working
example from which you can start. Once completed, you can start adding
your own types to the tutorial. And once they start working, you can
"take over" the system and gradually remove the Star Wars parts until
you have a fully working example.

This implementation backs the system by means of a Mnesia database.
The choice is deliberate for a couple of reasons:

* Mnesia is present in any Erlang system and thus it provides a simple
  way to get started and setup.
* Mnesia is *not* a Graph Database. This makes it explicit your
  database can be anything. In fact, the "Graph" in GraphQL is
  misnomer since GraphQL works even when your data does not have a
  typical Graph-form. It is simply a nice query structure.

=== What we do not cover

This tutorial doesn't cover everything in the repository:

* The details of the `rebar3` integration and the `relx` release
  handling.
* The tutorial only covers the parts of the code where there is
  something to learn. The areas of the code getting exposition in this
  document is due to the fact that they convey some kind of important
  information about the use of the GraphQL system for Erlang. Other
  parts, which are needed for completeness, but aren't as important
  are skipped.
* There is no section on "`how do I set up an initial Erlang
  environment`" as it is expected to be done already.

=== Overview

The purpose of a GraphQL server is to provide a contract between a
client and a server. The contract ensures that the exchange of
information follows a specific structure, and that queries and
responses are in accordance with the contract specification.

Additionally, the GraphQL servers contract defines what kind of
queries are possible and what responses will look like. Every query
and response is typed and a type checker ensures correctness of data.

Finally, the contract is introspectable by the clients. This allows
automatic deduction of queries and built-in documentation of the
system interface.

Thus, a GraphQL server is also a contract checker. The GraphQL system
ensures that invalid queries are rejected, which makes it easier to
implement the server side: you can assume queries are valid to a far
greater extent than is typical in other systems such as typical REST
interfaces.

=== Plan

In order to get going, we need a world in which to operate. First, we
must provide two schemas: one for the GraphQL system, and one for the
Mnesia database.

The GraphQL schema defines the client/server contract. It consists of
several GraphQL entity kinds. For example:

* Scalar types--Extensions on top of the default types. Often used
  for Dates, DateTime's, URI's, Color's, Currency, Locales and so on.
* Enumerations--Values taken from a limited set. An example could be
  the enumeration of weekdays: "`MONDAY, TUESDAY, WEDNESDAY, ...,
  SUNDAY`".
* Input Objects--Data flowing from the Client to the Server (Request).
* Output Objects--Data flowing from the Server to the Client
  (Response).

A somewhat peculiar choice by the GraphQL authors is that the world of
Input and Output objects differ. In general, a Client has no way to
"_PUT_" an input object back into the Graph as is the case in REST
systems. From a type-level perspective, client requests and server
responses have different _polarity_.

It may seem as if this is an irritating choice. You often have to
specify the "`same`" object twice: once for input and once for output.
However, as your GraphQL systems grows in size, it turns out this
choice is the right one. You quickly run into situations where a
client supplies a desired specific change where many of the fields on
the output object doesn't make sense. By splitting the input and
output world, it is easily to facilitate since the input objects can
omit many fields that doesn't make sense.

In a way, your GraphQL system is built such that changes to the data
is done by executing "`transactions`" through a set of stored
procedures. This can be seen as using the "`_PATCH_`" method of RESTful
interfaces and not having a definition of PUT.

.CQRS

GraphQL splits the schema into two worlds: _query_ and _mutation_. The
difference from the server side is mostly non-existent: the GraphQL
system is allowed to parallelize queries but not mutations. But from
the perspective of the client, the starting points in the graph is
either the _query_ or the _mutation_ object.

CQRS stands for Command-Query Responsiblity Separation. The idea stems
from the observation that querying data often have a different feel
than commanding the system to do changes. So rather than trying to
solve both in one interface, you slice the system such that you have a
query-part which pertains only to querying data, and a command-part
which pertains to mutating data.

GraphQL implements what is essentially CQRS by making a distinction
between the notion of a _query_ and a _mutation_. Likewise, the server
side makes this distinction. But on the server side it is merely
implemented by having different starting objects in the graph
execution.

.Mnesia

Our Star Wars schema uses the database *mnesia* as a backend. It is
important to stress that you often have a situation where your
database backend doesn't map 1-1 onto the GraphQL schema you specify.
In larger systems, this is particularly important: the GraphQL schema
is often served by multiple different backends, and those backends are
not going to cleanly map onto the world we expose to the clients. So
the GraphQL schema contract becomes a way to mediate between the
different data stores. As an example, you may satisfy some parts of
the GraphQL from a dedicated search system--such as
ElasticSearch--while others are served as rows from a traditional
database, such as MySQL or Postgresql. You may even have a message
queue broker or some other subsystem in which you have relevant data
you want to query. Or perhaps, some queries are handled by
micro-services in your architecture.

Over the course of having built larger systems, we've experienced that
mappings which tries to get isomorphism between the backend and the
schema creates more problems than they solve. Small changes have
consequence in all of the stack. Worse, you can't evolve part of the
system without evolving other parts which impairs the flexibility of
the system.

Another problem is that you may end up with an impedence mismatch
between the Objects and links of the GraphQL query and the way you
store your data in the backend. If you force a 1-1 relationship
between the two, you can get into trouble because your GraphQL schema
can't naturally describe data.

.Mnesia initialization

A common problem people run into with Mnesia is how to "`get started`".
What people often resort to are solutions where an initial database is
created if it doesn't exist. These solutions are often brittle.

Here, we pick another solution. A helper can create a database schema
for us, with all the necessary tables. The real release _assumes_ the
presence of an initial database and won't boot without one. This means
the Erlang release is simpler. There is always some database from
which it can boot and operate. That database might be the empty
database since we are just starting out. But in particular, the
release won't concern itself with creating an initial database. Rather
it will assume one is already existing.

The situation is not much different than using a traditional
schema-oriented database. Usually, you have to create the database
first, and then populate the schema with some initial data. It is just
because of Rails/Django like systems in which databases are
migrate-established, we've started using different models.

== Mnesia
=== Setting up an initial mnesia schema

To get up and running, we begin by constructing a mnesia schema we can
start from. We do this by starting a shell on the Erlang node and then
asking it to create the schema:

[source]
----
$ git clean -dfxq # <1>
$ make compile # <2>
$ make shell-schema # <3>
erl -pa `rebar3 path` -name sw@127.0.0.1
Erlang/OTP 19 [erts-8.3] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false] [dtrace]

Eshell V8.3  (abort with ^G)
1> sw_core_db:create_schema(). % <4>
----
<1> Clean out the source code repository to make sure there is no lingering files
<2> Compile the code so we have compiled versions of modules we can loaded
<3> Run the erlang interpreter with an altered path for our newly compiled modules
<4> Create the schema

The call the `create_schema()` runs the the following schema creation code:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.erl[tags=createSchema]
----

Creating the schema amounts to running a set of commands from the
mnesia documentation. The helper function to create tables contains a
large number of tables, so we are just going to show one here:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.erl[tags=createTables]
----

In mnesia, tables are Erlang records. The `#planet{}` record needs
definition and is in the header file `sw_core_db.hrl`. We simply list
the entries which are defined the SWAPI GraphQL schema so we can store
the concept of a starship in the system:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.hrl[tags=planetRecord]
----

Every other table in the system is handled in the same manner, but are
not given here for brevity. They follow the same style as the example above.

=== Populating the database

Once we have introduced tables into the system, we can turn our
attention to populating the database tables. For this, we use the
SWAPI data set as the primary data source. This set has its fixtures
stored as JSON document. So we use `jsx` to decode those JSON
documents and turn them into mnesia records, which we then insert into
the database.

We can fairly easily write a function which take the JSON and turn them
into appropriate mnesia records. Planets live in a fixture file
`planets.json`, so we need a function that can turn a part said JSON
file into a mnesia record. Some conversion is necessary on the way:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.erl[tags=json_to_planet]
----

Once we have this function down, we can utilize it to get a list of
mnesia records, which we can then insert into the database through a
transaction:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.erl[tags=populate_planets]
----

The code to read in and populate the database is fairly
straightforward. It is the last piece of the puzzle to inject relevant
data into the Mnesia database:

[source,erlang]
----
include::{sw_core}/src/sw_core_db.erl[tags=populatingTables]
----


This creates a fixture in the database such that when we boot the
database, the transport and starship data will be present in the
records of the starship table.

=== Creating a FALLBACK for the database

Once we have run the schema creation routine, a file called
`FALLBACK.BUP` is created. We copy this to the database base core in
the repository
[source,bash]
----
$ cp FALLBACK.BUP db/FALLBACK.BUP
----
which makes the empty schema available for the release manager of the
Erlang system. When we cook a release, we will make sure to copy this
initial schema into the correct mnesia-directory of the release.
Because the file is named `FALLBACK.BUP`, it is a fallback backup file.
This will "`unpack`" itself to become a new empty database as if you
had rolled in a backup on the first boot of the system. Thus we avoid
our system having to deal with this problem at start up.

NOTE: A real system will override the location of the Mnesia `dir`
parameter and define a separate directory from which the Mnesia
database will run. Initially, the operator will place the
`FALLBACK.BUP` file in this directory to get going, but once we are
established, and people start adding in data, we can't reset anything
when deploying new versions. Hence the separate directory so we can
upgrade the Erlang system without having to protect the database as
much.

We now have the ability to create new database tables easily and we
have a mnesia database for backing our data. This means we can start
turning our attention to the GraphQL schema.
